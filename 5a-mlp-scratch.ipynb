{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons from Scratch\n",
    "\n",
    "Now that we've covered all the preliminaries, extending to deep neural networks is actually quite easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data (surprise!)\n",
    "\n",
    "Let's go ahead and grab our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = mx.test_utils.get_mnist()\n",
    "batch_size = 64\n",
    "train_data = mx.io.NDArrayIter(mnist[\"train_data\"], mnist[\"train_label\"], batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"], batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons\n",
    "\n",
    "Here's where things start to get interesting. Before, we mapped our inputs directly onto our outputs through a single linear transformation. \n",
    "\n",
    "![](img/simple-softmax-net.png)\n",
    "\n",
    "This model is perfectly adequate when the underlying relationship between our data points and labels is approximately linear. When our data points and targets are characterized by a more complex relationship, a linear model and produce sucky results. We can model a more general class of functions by incorporating one or more *hidden layers*.\n",
    "\n",
    "![](img/multilayer-perceptron.png)\n",
    "\n",
    "Here, each layer will require it's own set of parameters. To make things simple here, we'll assume two hidden layers of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = nd.random_normal(shape=(784,256)) *.01\n",
    "b1 = nd.random_normal(shape=256) * .01\n",
    "\n",
    "W2 = nd.random_normal(shape=(256,128)) *.01\n",
    "b2 = nd.random_normal(shape=128) * .01\n",
    "\n",
    "W3 = nd.random_normal(shape=(128,10)) *.01\n",
    "b3 = nd.random_normal(shape=10) *.01\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's allocate space for gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "If we compose a multi-layer network but use only linear operations, then our entire network will still be a linear function. That's because $\\hat{y} = X \\cdot W_1 \\cdot W_2 \\cdot W_2 = X \\cdot W_4 $ for $W_4 = W_1 \\cdot W_2 \\cdot W3$. To give our model the capacity to capture nonlinear functions, we'll need to interleave our linear operations with activation functions. In this case, we'll use the rectified linear unit (ReLU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X,nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Outputs\n",
    "\n",
    "As with multiclass logistic regression, we'll want out outputs to be *stochastic*, meaning that they constitute a valid probability distribution. We'll use the same softmax activation functino on our output to make sure that our outputs sum to one and are non-negative.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear)\n",
    "    partition =nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "Now we're ready to define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    h1_linear = nd.dot(X, W1) + b1\n",
    "#     print(\"h1_linear shape: %s\" % (np.array(h1_linear.shape)))\n",
    "    h1 = relu(h1_linear)\n",
    "#     print(\"h1 shape: %s\" % (np.array(h1.shape)))\n",
    "    h2_linear = nd.dot(h1, W2) + b2\n",
    "#     print(\"h2_linear shape: %s\" % (np.array(h2_linear.shape)))\n",
    "    h2 = relu(h2_linear)\n",
    "    yhat_linear = nd.dot(h2, W3) + b3\n",
    "    yhat = softmax(yhat_linear)\n",
    "#     print(yhat.shape)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cross-entropy Loss Function\n",
    "\n",
    "Nothing changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - nd.sum(y * nd.log(yhat), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    \n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        with autograd.record():\n",
    "            data = batch.data[0].as_in_context(ctx).reshape((-1,784))\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            label_one_hot = nd.one_hot(label, 10)\n",
    "            output = net(data)\n",
    "        \n",
    "        metric.update([label], [output])\n",
    "    return metric.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.450860380328, Train_acc 0.865368150685, Test_acc 0.868929140127\n",
      "Epoch 1. Loss: 0.289160527269, Train_acc 0.893842751142, Test_acc 0.872591353834\n",
      "Epoch 2. Loss: 0.227494174882, Train_acc 0.911334665145, Test_acc 0.897242490413\n",
      "Epoch 3. Loss: 0.17750015641, Train_acc 0.923448202055, Test_acc 0.913436047356\n",
      "Epoch 4. Loss: 0.136533414566, Train_acc 0.932308789954, Test_acc 0.924822983249\n",
      "Epoch 5. Loss: 0.108078708505, Train_acc 0.939017313546, Test_acc 0.933277476918\n",
      "Epoch 6. Loss: 0.0902558591182, Train_acc 0.944353392042, Test_acc 0.939694979188\n",
      "Epoch 7. Loss: 0.076760384345, Train_acc 0.948783533105, Test_acc 0.944891012529\n",
      "Epoch 8. Loss: 0.0676438058201, Train_acc 0.952449581431, Test_acc 0.949198160816\n",
      "Epoch 9. Loss: 0.0561751382824, Train_acc 0.955552226027, Test_acc 0.952783222633\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        with autograd.record():\n",
    "            data = batch.data[0].as_in_context(ctx).reshape((-1,784))\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            label_one_hot = nd.one_hot(label, 10)\n",
    "            output = net(data)\n",
    "            loss = cross_entropy(output, label_one_hot)\n",
    "            loss.backward()\n",
    "        SGD(params, .001)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if i == 0:\n",
    "            moving_loss = np.mean(loss.asnumpy()[0])\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
    "            \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: [ 64 784]\n"
     ]
    }
   ],
   "source": [
    "print(\"data shape: %s\" % (np.array(data.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Noice. With just two hidden layers containing 256 and 128 hidden nodes, repsectively, we can achieve over 95% accuracy on this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
