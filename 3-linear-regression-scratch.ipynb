{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch\n",
    "\n",
    "While powerful libraries remove some of the repetitive work in deep learning, they can the important details you need to really understand whaat's going on under the hood. So in this first example, we won't touch any of the MXNet's more abstract features. Instead we'll rely only on autograd and NDArray, building everything else from scratch.\n",
    "\n",
    "First, we'll import a few dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "\n",
    "We'll focus on the provlem of linear regression. Given a collection of data points ``X``, and corresponding target values ``y``, we'll try to find the line, parameterized by a vector ``w`` and intercept ``b`` that approximately fits ``y = Xw + b``. \n",
    "\n",
    "To make things easy, we're going to work with a synthetic data where we know the solution, by generating random data points ``X[i]`` and labels ``y[i] = 2 * X[i][0]- 3.4 * X[i][1] + 4.2 + noise`` where the noise is drawn from a random gaussian with mean ``0`` and variance ``.1``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(10000,2)\n",
    "Y = 2* X[:,0] - 3.4 * X[:,1] + 4.2 + .01 * np.random.normal(size=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each row in ``X`` consists of a 2-dimensional data point and that each row in ``Y`` consists of a 1-dimensional target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79406084  1.56172636]\n",
      "0.456769317476\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that for any randomly chosen point, a linear combination with the *known* optimal parameters produces a prediction that is indeed close to the target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.478252050689\n"
     ]
    }
   ],
   "source": [
    "print(2 * X[0,0] - 3.4 * X[0,1] + 4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlBIM+EREKcGAT0SUEgz4REQp8f8BWNBstvAAVLIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122e62780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:,1],Y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Iterators\n",
    "\n",
    "Once we start working with neural networks, we're going to need to iterate through our data points quickly. We'll also want to be able to grab batches of ``k`` data points at a time, to shuffle our data. In MXNet, data iterators give us a nice set of utilities for fetching and manipulating data. In particular, we'll work with the simple  ``NDArrayIter`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = mx.io.NDArrayIter(X, Y, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've initialized our NDArrayIter (``train_data``), we can easily fetch batches by calling ``train_data.next()``. ``batch.data`` gives us a list of inputs. Because our model has only one input (``X``), we'll just be grabbing ``batch.data[0]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.04417992 -0.68918705]\n",
      " [ 1.9054625   0.60465395]\n",
      " [ 0.84493148 -1.56466305]\n",
      " [ 0.86293334 -2.31283331]]\n",
      "<NDArray 4x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "batch = train_data.next()\n",
    "print(batch.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also grab the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.63568211   5.9554739   11.21246243  13.77705765]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(batch.label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can iterate over ``train_data`` just as through it were an ordinary Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "train_data.reset()\n",
    "for i, batch in enumerate(train_data):\n",
    "    counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that we called ``train_data.reset()`` before iterating through it. This let's the iterator know to reshuffle the data, preparing for the next pass. See what happens if we try to pass over the data again without first hitting ``reset()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, batch in enumerate(train_data):\n",
    "    counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "Now let's allocate some memory for our parameters and set their initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = nd.random_normal(shape=(2,1))\n",
    "b = nd.random_normal(shape=1)\n",
    "\n",
    "params = [w, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the succeeding cells, we're going to update these parameters to better fit our data. That will involve taking the gradient (a multi-dimensional derivative) of some *loss function* with respect to the parameters. We'll update each parameter in the direction that reduces the loss. But first, let's just allocate some memory for each gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Next we'll want to define our model. In this case, we'll be working with linear models, the simplest possible *useful* neural network. To calculate the output of the linear model, we simply multipy a given input with the model's weights (``w``), and add the offset ``b``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X): \n",
    "    return nd.dot(X, w) + b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that was easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Train a model means making it better and better over the course of a period of training. But in order for this goal to make any sense at all, we first need to define what *better* means in the first place. In this case, we'll use the squared distance between our prediction and the true value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_loss(yhat, y): \n",
    "    return nd.mean((yhat - y) * (yhat-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "It turns out that linear regression actually has a closed-form solution. However, most interesting models that we'll care about cannot be solved analytically. So we'll solve this problem by stochastic gradient descent. At each step, we'll estimate the gradient of the loss with respect to our weights, using one batch randomly drawn from our dataset. Then, we'll update our parameters a small amount in the direction that reduces the loss. The size of the step is determined by the *learning rate* ``lr``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "Now that we have all the pieces all we have to do is wire them together by writing a training loop. First we'll define ``epochs``, the number of passes to make over the dataset. Then for each pass, we'll iterate through ``train_data``, grabbing batches of examples and their corresponding labels. \n",
    "\n",
    "For each batch, we'll go through the following ritual:\n",
    "* Generate predictions (``yhat``) and the loss (``loss``) by executing a forward pass through the network.\n",
    "* Calculate gradients by making a backwards pass through the network (``loss.backward()``). \n",
    "* Update the model parameters by invoking our SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving avg of loss: 50.9144\n",
      "Moving avg of loss: 0.794248208121\n",
      "Moving avg of loss: 0.0148730737706\n",
      "Moving avg of loss: 0.0025577\n",
      "Moving avg of loss: 0.000143381904767\n",
      "Moving avg of loss: 0.000106902367264\n",
      "Moving avg of loss: 4.256e-05\n",
      "Moving avg of loss: 0.000102382550679\n",
      "Moving avg of loss: 0.000106838756914\n",
      "Moving avg of loss: 4.37348e-05\n",
      "Moving avg of loss: 0.000102359822443\n",
      "Moving avg of loss: 0.000106844763359\n",
      "Moving avg of loss: 4.37431e-05\n",
      "Moving avg of loss: 0.000102359376285\n",
      "Moving avg of loss: 0.000106844775341\n",
      "Moving avg of loss: 4.37431e-05\n",
      "Moving avg of loss: 0.000102359376285\n",
      "Moving avg of loss: 0.000106844775341\n",
      "Moving avg of loss: 4.37431e-05\n",
      "Moving avg of loss: 0.000102359376285\n",
      "Moving avg of loss: 0.000106844775341\n",
      "Moving avg of loss: 4.37431e-05\n",
      "Moving avg of loss: 0.000102359376285\n",
      "Moving avg of loss: 0.000106844775341\n",
      "Moving avg of loss: 4.37431e-05\n",
      "Moving avg of loss: 0.000102359376285\n",
      "Moving avg of loss: 0.000106844775341\n",
      "Moving avg of loss: 4.37431e-05\n",
      "Moving avg of loss: 0.000102359376285\n",
      "Moving avg of loss: 0.000106844775341\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "ctx = mx.cpu()\n",
    "moving_loss = 0.\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        with autograd.record():\n",
    "            data = batch.data[0].as_in_context(ctx)\n",
    "            label = batch.label[0].as_in_context(ctx).reshape((-1,1))\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "#             print(W.grad)\n",
    "            loss.backward()\n",
    "        SGD(params, .001)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if i == 0:\n",
    "            moving_loss = np.mean(loss.asnumpy()[0])\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Moving avg of loss: %s\" % (moving_loss))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "You've seen that using just mxnet.ndarray and mxnet.autograd, we can build statistical models from scratch. In the following tutorials, we'll build on this foundation, introducing the basic ideas between modern neural networks and powerful abstractions in MXNet for building comples models with little code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
