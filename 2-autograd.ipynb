{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation with ``autograd`` \n",
    "\n",
    "\n",
    "In machine learning, we want models to get better and better as a function of experience. Usually *getting better*, means minimizing a *loss function*, a measure of how *bad* our model is at any time. With neural networks, that loss is usually differentiable, i.e. for each of the model's parameters, we can always determine how much increasing or decreasing it might affect the loss. For complex models, working out these derivatives from scratch can be a pain.\n",
    "\n",
    "_MXNet_'s autograd package eliminates this tedious work by automatically calculating derivatives for you. Other libraries require you to pre-define and compile symbolic graphs in order to access automatic derivatives. However, ``autograd``, much like the similar package in PyTorch, allows you to take derivatives even when running fully imperative code.\n",
    "\n",
    "Essentially, every time you make pass through your model, autograd builds a graph on the fly, through which it can immediately backpropagate gradients.\n",
    "\n",
    "Let's go through it step by step. For this tutorial, we'll only need to import ``mxnet.ndarray``, and ``mxnet.autograd``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "from mxnet import autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaching Gradients\n",
    "\n",
    "As a toy example, Let's say that we are interested in differentiating a function ``f = 2 * (x ** 2)`` with respect to parameter x. We can start by assining an initial value of ``x``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = mx.nd.array([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we compute the gradient of ``f`` with respect to ``x``, we'll need a place to store it. In _MXNet_, we can tell an NDArray that we plan to store a gradient by invoking its ``atach_grad()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to define out function ``f`` and *MXNet* will generate a computation graph on the fly. It's as if *MXNet* turned on a recording device and captured the exact path by which each variabel was generated. \n",
    "\n",
    "Note that building the computation graph requires a nontrivial amount of computation. So we only *MXNet* to build the graph when explicitlt told to do so. We can instruct *MXNet* to start recording by placing code inside a ``with autograd.record():`` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "  y = x * 2\n",
    "  f = y * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's backprop with f.backward(). When f has more than one entry, f.backward() is equivalent to mx.nd.sum(f).backward().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if this is the expected output. Remember that ``y = x * 2``, and ``f = x * y``, so ``f`` should be equal to  ``2 * x * x``. After, doing backprop with ``f.backward()``, we expect to get back gradient df/dx as follows: dy/dx = ``2``, df/dx = ``4 * x``. So, if everything went according to plan, ``x.grad`` should consist of an NDArray with the values ``[[4, 8],[12, 16]]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.          8.        ]\n",
      " [ 1.20000005  1.60000002]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Gradients and the Chain Rule\n",
    "\n",
    "*Warning: This part is tricky, but not necessary to understanding subsequent sections.*\n",
    "\n",
    "Sometimes when we call the backward method on an NDArray, e.g. ``y.backward()``, where ``y`` is a function of ``x`` we are just interested in the derivative of ``y`` with respect to ``x``. At other times, we may be interested in the gradient of ``z`` with respect to ``x``, where ``z`` is a function of ``y``. Recall that by the train rule dz/dx can be expressed in terms of dz/dy and dy/dx. So, when ``y`` is part of a larger function ``z``, and we want ``x.grad`` to store dz/dx, we can pass in the *head gradient* dz/dy as an input to ``backward()``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 40.           8.        ]\n",
      " [  1.20000005   0.16      ]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "  y = x * 2\n",
    "  f = y * x\n",
    "    \n",
    "gradients = nd.array([[10,1.],[.1,.01]])\n",
    "f.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the basics, we can do some wild things with autograd, including building diferentiable functions using Pythonic control flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nd.random_normal(shape=3)\n",
    "a.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    b = a * 2\n",
    "    while (nd.norm(b) < 1000).asscalar():\n",
    "        b = b * 2\n",
    "\n",
    "    if (mx.nd.sum(b) > 0).asscalar():\n",
    "        c = b\n",
    "    else :\n",
    "        c = 100 * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "[20:02:50] src/ndarray/autograd.cc:157: Check failed: !i.entry_.is_none() Cannot differentiate node because it is not in a computational graph. You need to set is_training to true or use autograd.record() to save computational graphs for backward. If you want to differentiate the same graph twice, you need to pass retain_graph=True to backward.\n\nStack trace returned 5 entries:\n[bt] (0) 0   libmxnet.so                         0x0000000108e458a5 _ZN4dmlc15LogMessageFatalD2Ev + 37\n[bt] (1) 1   libmxnet.so                         0x000000010967f59b _ZN5mxnet8autograd15AutogradRuntime15ComputeGradientERKNSt3__16vectorINS_7NDArrayENS2_9allocatorIS4_EEEES9_b + 11131\n[bt] (2) 2   libmxnet.so                         0x00000001095b873e MXAutogradBackward + 494\n[bt] (3) 3   _ctypes.cpython-36m-darwin.so       0x00000001032412c7 ffi_call_unix64 + 79\n[bt] (4) 4   ???                                 0x00007fff5e460560 0x0 + 140734775035232\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-72bcf67cb3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/liptoz/workspace/mxnet/python/mxnet/ndarray.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, out_grad, retain_graph)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNDArrayHandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNDArrayHandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mograd_handles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             ctypes.c_int(retain_graph)))\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liptoz/workspace/mxnet/python/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \"\"\"\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [20:02:50] src/ndarray/autograd.cc:157: Check failed: !i.entry_.is_none() Cannot differentiate node because it is not in a computational graph. You need to set is_training to true or use autograd.record() to save computational graphs for backward. If you want to differentiate the same graph twice, you need to pass retain_graph=True to backward.\n\nStack trace returned 5 entries:\n[bt] (0) 0   libmxnet.so                         0x0000000108e458a5 _ZN4dmlc15LogMessageFatalD2Ev + 37\n[bt] (1) 1   libmxnet.so                         0x000000010967f59b _ZN5mxnet8autograd15AutogradRuntime15ComputeGradientERKNSt3__16vectorINS_7NDArrayENS2_9allocatorIS4_EEEES9_b + 11131\n[bt] (2) 2   libmxnet.so                         0x00000001095b873e MXAutogradBackward + 494\n[bt] (3) 3   _ctypes.cpython-36m-darwin.so       0x00000001032412c7 ffi_call_unix64 + 79\n[bt] (4) 4   ???                                 0x00007fff5e460560 0x0 + 140734775035232\n"
     ]
    }
   ],
   "source": [
    "gradients = nd.array([0.01,1.0,.1])\n",
    "c.backward(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.          8.        ]\n",
      " [ 1.20000005  1.60000002]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
