{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plumbing: A look under the hood of ``mxnet.gluon``\n",
    "\n",
    "In the previous tutorials, we taught you about linear regression and softmax regression. We explained how these models work in principle, showed you how to implement them from scratch, and presented a compact implementation using ``mxnet.gluon``. And since our focus was on modeling, we showed \n",
    "\n",
    "We explained *how to do things* in ``gluon`` but didn't really explain *how they work*. We relied on ``nn.Sequential``, syntactically convenient shorthand for ``nn.Block`` but didn't peek under the hood.  And while each notebook presented a working, trained model, we didn't show you how to introspect its parameters, save and load models, etc. In this chapter, we'll take a break from modeling to explore the gory details of ``mxnet.gluon``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the data\n",
    "First, let's get the preliminaries out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "ctx = mx.cpu()\n",
    "batch_size = 64\n",
    "mnist = mx.test_utils.get_mnist()\n",
    "train_data = mx.io.NDArrayIter(\n",
    "    mnist[\"train_data\"], \n",
    "    mnist[\"train_label\"], \n",
    "    batch_size, \n",
    "    shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(\n",
    "    mnist[\"test_data\"], \n",
    "    mnist[\"test_label\"], \n",
    "    batch_size, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peeling away the abstraction of ``nn.Sequential``\n",
    "Now you might remember that we defined a multilayer perceptron in gluon thusly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = gluon.nn.Sequential()\n",
    "with net1.name_scope():\n",
    "    net1.add(gluon.nn.Dense(128, activation=\"relu\"))\n",
    "    net1.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "    net1.add(gluon.nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In just 5 lines and 183 characters, we defined a multilayer perceptron with three fully-connected layers, each parametrized by weight matrix and bias term. We also specified the ReLU activation function for the hidden layers. The first time I had to implement a multilayer perceptron for a university machine learning course it took considerably more code. To enable such concise code, there's a bit of magic going on here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape inference\n",
    "One of the first things you might notice is that for each layer, we only specified the number of nodes output, we never specified how many input nodes! You might wonder, how does ``gluon`` know that the first weight matrix should be $784 \\times 128$ and not $42 \\times 128$. In fact it doesn't. We can see this by accessing the network's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential1_ (\n",
      "  Parameter sequential1_dense0_bias (shape=(128,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense2_weight (shape=(10, 0), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense2_bias (shape=(10,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense1_weight (shape=(64, 0), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense1_bias (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense0_weight (shape=(128, 0), dtype=<class 'numpy.float32'>)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net1.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the shapes of the weight matrices: (128,0), (64, 0), (10, 0). What does it mean to have zero dimension in a matrix? This is ``gluon``'s way of marking that the shape of these matrices is not yet known. The shape will be inferred on the fly once the network is provided with some input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we initialize our parameters, you might wonder, what precisely is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation, ``gluon`` is not actually initializing any parameters! Instead, it's making a note of which initializer to associate with each parameter, even though it's shape is not yet known. The parameters are instantiated and the initializer is called once we provide the network with some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential1_ (\n",
      "  Parameter sequential1_dense0_bias (shape=(128,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense2_weight (shape=(10, 64), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense2_bias (shape=(10,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense1_weight (shape=(64, 128), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense1_bias (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense0_weight (shape=(128, 784), dtype=<class 'numpy.float32'>)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data = train_data.next().data[0]\n",
    "net1(data)\n",
    "print(net1.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape inference can be extremely useful at times. For example, when working with convnets, it can be quite a pain to calculate the shape of various hidden layers. It depends on both the kernel size, the number of filters, the stride, and the precise padding scheme used which can vary in subtle ways from library to library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying shape manually\n",
    "\n",
    "If we want to specify the shape manually, that's always an option. We accomplish this by using the ``in_units`` argument when adding each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = gluon.nn.Sequential()\n",
    "with net2.name_scope():\n",
    "    net2.add(gluon.nn.Dense(784, in_units=128, activation=\"relu\"))\n",
    "    net2.add(gluon.nn.Dense(64, in_units=128, activation=\"relu\"))\n",
    "    net2.add(gluon.nn.Dense(10, in_units = 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the parameters from this network can be initialized before we see any real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential8_ (\n",
      "  Parameter sequential8_dense1_weight (shape=(64, 128), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential8_dense1_bias (shape=(64,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential8_dense0_bias (shape=(784,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential8_dense2_bias (shape=(10,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential8_dense0_weight (shape=(784, 128), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential8_dense2_weight (shape=(10, 64), dtype=<class 'numpy.float32'>)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net2.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "print(net2.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the deal with ``name_scope()``?\n",
    "The next thing you might have noticed is that we added all of our layers inside a ``with net1.name_scope():`` block. This coerces ``gluon`` to give each parameter an appropriate name, indicating which model it belongs to, e.g. ``sequential8_dense2_weight``. Keeping these names straight makes our lives much easier once we start writing more complex code where we might be working with multiple models and saving and loading the parameters of each. It helps us to make sure that we associate each weight with the right model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Behind ``Sequential``'s syntactic sugar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
