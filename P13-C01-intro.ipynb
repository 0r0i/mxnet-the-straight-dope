{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "You may often hear that training a deep learning model may take hours, days, or even weeks. \n",
    "\n",
    "![](img/training_model.png)\n",
    "\n",
    "\n",
    "## Why optimization matters\n",
    "\n",
    "Indeed, model training is often considered as the most challenging stage for a deep learning task. Essentially, for most machine learning models, the goal of training is to find values of model parameters that minimize a pre-defined loss function. \n",
    "\n",
    "Consider the task of classifying an image as either a cat or a dog. Suppose that the training data set consists of images labeled as cats and dogs. With extracted features such as pixel information representing each image, a deep learning model transforms such input features into the likelihoods of being a cat and a dog. Such transformations are essentially linear and nonlinear functions of both input features and model parameters. Note that the input features are given, while the model parameter values are unknown beforehand and shall be obtained after model training.\n",
    "\n",
    "We can train this model with maximum likelihood estimation. The idea is to maximize the joint likelihood of observing all the labeled cats and dogs in the training data set as transformations from the input image features and model parameters. Equivalently, we can define the loss function as the negative log likekihood of the observations in the training data set. We want to find the optimal values of the model parameters that minimize the loss function. To achieve this goal, we rely on optimization algorithms to find such model parameter values. Thus, it is fair to say that optimization plays a key role in training a deep learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization is usually hard\n",
    "\n",
    "The pre-defined loss function in the learning problem is called the objective function for optimization. Conventionally, optimization considers a minimization problem and maxinimizing an objective function can be trivally converted to minimizing the negative of the objective function.\n",
    "\n",
    "### Local and global minima\n",
    "\n",
    "An objective function $f(x)$ may have a local minumum $x$, where $f(x)$ is smaller at $x$ than at the neighboring points of $x$. If $f(x)$ is the smallest value that can be obtained in the entire domain of $x$, $f(x)$ is a global mininum.\n",
    "The following figure demonstrates examples of local and global minima for function \n",
    "\n",
    "$$f(x) = x \\cdot \\text{cos}(\\pi x), \\qquad -1.0 \\leq x \\leq 2.0.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x * np.cos(np.pi * x)\n",
    "\n",
    "x = np.arange(-1.0, 2.0, 0.1)\n",
    "fig = plt.figure()\n",
    "subplt = fig.add_subplot(111)\n",
    "subplt.annotate('local minimum', xy=(-0.3, -0.2), xytext=(-0.8, -1.0),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "subplt.annotate('global minimum', xy=(1.1, -0.9), xytext=(0.7, 0.1),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.plot(x, f(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, it would be ideal if we can find the solution $x$ that globally minimizes an objective function. For instance, function $f(x) = x^2$ has a global minimum solution at $x = 0$. This is an analytical solution that can be obtained by calculus.\n",
    "\n",
    "However, in practice finding such analytical solutions is not always feasible. The function $f(x) = x \\cdot \\text{cos}(\\pi x), -1.0 \\leq x \\leq 2.0$ plotted above is an example of such cases. When an analytical solution is not available, we often resort to a numerical solution that is usually obtained by computers in an iterative manner. A large number of iterations may be needed to make the numerical solution useful. That is why model training may take a long time.\n",
    "\n",
    "With a numerical method, it is usually hard to find the global minimum solution to an objective function. For non-convex functions, a numerical method often halt around local minima that are not necessarily the global minima. Other challenging scenarios include the existence of many saddle points surrounded by flat regions. A saddle point example is shown as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-2.0, 2.0, 0.1)\n",
    "fig = plt.figure()\n",
    "subplt = fig.add_subplot(111)\n",
    "subplt.annotate('saddle point', xy=(0, -0.2), xytext=(-0.4, -5.0), \n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.plot(x, x**3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine precision constraint\n",
    "\n",
    "Even though for convex functions where local minima are global ones, it may still be hard to find the precise optimal solutions. A solution can be constrained by the machine precision.\n",
    "\n",
    "In computers, numbers are represented in a discrete manner. The accuracy of a floating-point system is characterized by a quantity called machine precision. For IEEE binary floating-point systems, \n",
    "\n",
    "* single precision = $2^{-24}$ (about 7 decimal digits of precision)\n",
    "* double precision = $2^{-53}$ (about 16 decimal digits of precision).\n",
    "\n",
    "In fact, the precision of a solution to optimization can be worse than the machine precision. To demonstrate that, consider a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, its Taylor series exansion is\n",
    "\n",
    "$$f(x + \\epsilon) = f(x) + f'(x)\\epsilon + \\frac{f''(x)}{2}\\epsilon^2 + \\mathcal{O}(\\epsilon^3).$$\n",
    "\n",
    "Denote the global optimum solution as $x^*$ for minimizing $f(x)$. It usually holds that \n",
    "\n",
    "$$f'(x^*) = 0 \\quad \\text{and} \\quad f''(x^*) \\neq 0.$$\n",
    "\n",
    "Thus, for a small value $h$, we have\n",
    "\n",
    "$$f(x^* + \\epsilon) \\approx f(x^*) + \\mathcal{O}(\\epsilon^2),$$\n",
    "\n",
    "where the coefficient term of $\\mathcal{O}(\\epsilon^2)$ is $f''(x)/2$. It means that a small change of order $\\epsilon$ in the optimum solution $x^*$ will change the value of $f(x^*)$ in the order of $\\epsilon^2$. In other words, if there is an error in the function value, the precision of solution value is constrained by the order of the square root of that error. For example, if the machine precision is $10^{-8}$, the precision of the solution value is only in the order of $10^{-4}$, which is much worse than the machine precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding global optima is not necessary for deep learning\n",
    "\n",
    "Although finding the precise global optimum solution to the objective function is hard, it is not necessary for deep learning. In practice, we often aim at finding the solution that makes the objective function very low, and such solutions are not necessarily the global optima.\n",
    "\n",
    "Indeed, many algorithms have solid theoretical guarantees that only encourage their applications to objective functions that are convex. In old times, it was often considered unprincipled to apply such algorithms to nonconvex problems. However, nowadays many deep learning models have found such optimization algorithms helpful. They are able to quickly find solutions that make the objective function low enough to have useful applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
