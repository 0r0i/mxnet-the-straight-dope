{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization from scratch\n",
    "\n",
    "[Batch Normalization](https://arxiv.org/abs/1502.03167) is another way to avoid overfitting, gradient explosion or elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "mx.random.seed(1)\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "Let's prepare the data (as always!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization layer\n",
    "\n",
    "The layer, unlike Dropout, is usually used **before** the activation layer \n",
    "(according to the authors' original paper), instead of after activation layer.\n",
    "\n",
    "The basic idea is doing the normalization then applying a linear scale and shift to the mini-batch:\n",
    "\n",
    "For input mini-batch $B = \\{x_{1, ..., m}\\}$, we want to learn the parameter $\\gamma$ and $\\beta$.\n",
    "The output of the layer is $\\{y_i = BN_{\\gamma, \\beta}(x_i)\\}$, where:\n",
    "\n",
    "$$\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m}x_i$$\n",
    "$$\\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(x_i - \\mu_B)^2$$\n",
    "$$\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "$$y_i \\leftarrow \\gamma \\hat{x_i} + \\beta \\equiv \\mbox{BN}_{\\gamma,\\beta}(x_i)$$\n",
    "\n",
    "* formula taken from Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" International Conference on Machine Learning. 2015.\n",
    "\n",
    "For the spirit of \"from scratch\", we implement the layer by ourselves,\n",
    "by referencing the formulas from the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pure_batch_norm(X, gamma = 1, beta = 0, eps = 1e-5):\n",
    "    # mini-batch mean\n",
    "    mu = nd.mean(X, axis=0)\n",
    "    \n",
    "    # mini-batch variance\n",
    "    variance = nd.mean((X - mu) ** 2, axis=0)\n",
    "    \n",
    "    # normalize\n",
    "    X_hat = (X - mu) * 1.0 / nd.sqrt(variance + eps)\n",
    "    \n",
    "    # scale and shift\n",
    "    out = gamma * X_hat + beta\n",
    "    \n",
    "    # return\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some sanity check. We expect each **column** of the input matrix is normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  1.   7.]\n",
       " [  5.   4.]\n",
       " [  6.  10.]]\n",
       "<NDArray 3x2 @gpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = nd.array([1,7,5,4,6,10], ctx=ctx).reshape((3,2))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-1.38872862  0.        ]\n",
       " [ 0.46290955 -1.22474384]\n",
       " [ 0.9258191   1.22474384]]\n",
       "<NDArray 3x2 @gpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(A,\n",
    "    gamma = nd.array([1,1], ctx=ctx), \n",
    "    beta=nd.array([0,0], ctx=ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we implement it correctly.\n",
    "\n",
    "Note: Batch Normalization's **backward** pass is a little bit tricky. But we are not covering it here, because the `autograd` from `mxnet` automatically figures it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the testing process, we want to use the mean and variance of the **complete dataset**, instead of those of **mini batches**. In the implementation, we use moving statistics as a trade off, because we don't want to or don't have the ability to compute the statistics of the complete dataset (in the second loop).\n",
    "\n",
    "We need to improve the BN function a little bit.\n",
    "\n",
    "Then here comes another concern: we need to maintain the moving statistics **along with multiple runs of the BN**. It's an engineering issue rather than a deep/machine learning issue. On the one hand, the moving statistics are similar to `gamma` and `beta`; on the other hand, they are **not** updated by the gradient backwards. In this quick-and-dirty implementation, we make good use of a python feature: the statistics are stored as an dictionary attribute of the function, with `scope_name`s of each different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the attribute of a function in Python? Look at this example to get a feel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def func_with_state():\n",
    "    try:\n",
    "        func_with_state.something += 1\n",
    "    except:\n",
    "        func_with_state.something = 0\n",
    "    print(func_with_state.something)\n",
    "\n",
    "for _ in range(5):\n",
    "    func_with_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our `batch_norm()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(X,\n",
    "               gamma = 1,\n",
    "               beta = 0,\n",
    "               momentum = 0.9,\n",
    "               eps = 1e-5,\n",
    "               scope_name = '',\n",
    "               is_training = True,\n",
    "               debug = False):\n",
    "    \"\"\"compute the batch norm \"\"\"\n",
    "    #########################\n",
    "    # the usual batch norm transformation\n",
    "    #########################\n",
    "    \n",
    "    # mini-batch mean\n",
    "    mean = nd.mean(X, axis=0)\n",
    "    \n",
    "    # mini-batch variance\n",
    "    variance = nd.mean((X - mean) ** 2, axis=0)\n",
    "    \n",
    "    # normalize\n",
    "    if is_training:\n",
    "        # while training, we normalize the data using its mean and variance\n",
    "        X_hat = (X - mean) / nd.sqrt(variance + eps)\n",
    "    else:\n",
    "        # while testing, we normalize the data using the pre-computed mean and variance\n",
    "        X_hat = (X - batch_norm.moving_mean[scope_name]) / nd.sqrt(batch_norm.moving_var[scope_name] + eps)\n",
    "    \n",
    "    # scale and shift\n",
    "    out = gamma * X_hat + beta\n",
    "      \n",
    "    #########################\n",
    "    # to keep the moving statistics\n",
    "    #########################\n",
    "    \n",
    "    # init the attributes\n",
    "    try: # to access them\n",
    "        batch_norm.moving_mean\n",
    "        batch_norm.moving_var\n",
    "    except: # error, create them\n",
    "        batch_norm.moving_mean = {}\n",
    "        batch_norm.moving_var = {}\n",
    "    \n",
    "    # store the moving statistics by their scope_names, inplace    \n",
    "    if scope_name not in batch_norm.moving_mean:\n",
    "        batch_norm.moving_mean[scope_name] = mean\n",
    "    else:\n",
    "        batch_norm.moving_mean[scope_name] = batch_norm.moving_mean[scope_name] * momentum + mean * (1.0 - momentum)\n",
    "    if scope_name not in batch_norm.moving_var:\n",
    "        batch_norm.moving_var[scope_name] = variance\n",
    "    else:\n",
    "        batch_norm.moving_var[scope_name] = batch_norm.moving_var[scope_name] * momentum + variance * (1.0 - momentum)\n",
    "        \n",
    "    #########################\n",
    "    # debug info\n",
    "    #########################\n",
    "    if debug:\n",
    "        print('== info start ==')\n",
    "        print('scope_name = {}'.format(scope_name))\n",
    "        print('mean = {}'.format(mean))\n",
    "        print('var = {}'.format(variance))\n",
    "        print('moving_mean = {}'.format(moving_mean[scope_name]))\n",
    "        print('moving_var = {}'.format(moving_var[scope_name]))\n",
    "        print('output = {}'.format(out))\n",
    "        print('== info end ==')\n",
    " \n",
    "    #########################\n",
    "    # return\n",
    "    #########################\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = nd.random_normal(shape=(num_inputs, 256), ctx=ctx) *.01\n",
    "b1 = nd.random_normal(shape=256, ctx=ctx) * .01\n",
    "\n",
    "gamma1 = nd.random_normal(loc = 1, scale = .01, shape=256, ctx=ctx)\n",
    "beta1 = nd.random_normal(shape=256, ctx=ctx) * .01\n",
    "\n",
    "W2 = nd.random_normal(shape=(256,128), ctx=ctx) *.01\n",
    "b2 = nd.random_normal(shape=128, ctx=ctx) * .01\n",
    "\n",
    "gamma2 = nd.random_normal(loc = 1, scale = .01, shape=128, ctx=ctx)\n",
    "beta2 = nd.random_normal(shape=128, ctx=ctx) * .01\n",
    "\n",
    "W3 = nd.random_normal(shape=(128, num_outputs), ctx=ctx) *.01\n",
    "b3 = nd.random_normal(shape=num_outputs, ctx=ctx) *.01\n",
    "\n",
    "params = [W1, b1, gamma1, beta1, W2, b2, gamma2, beta2, W3, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    partition = nd.nansum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *softmax* cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "We insert the BN layer right after each linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X, is_training=True, debug=False):\n",
    "    #######################\n",
    "    #  Compute the first hidden layer \n",
    "    #######################    \n",
    "    h1_linear = nd.dot(X, W1) + b1\n",
    "    h1_normed = batch_norm(h1_linear, gamma1, beta1, scope_name='bn1', is_training=is_training, debug=debug)\n",
    "    h1 = relu(h1_normed)\n",
    "    \n",
    "    #######################\n",
    "    #  Compute the second hidden layer\n",
    "    #######################\n",
    "    h2_linear = nd.dot(h1, W2) + b2\n",
    "    h2_normed = batch_norm(h2_linear, gamma2, beta2, scope_name='bn2', is_training=is_training, debug=debug)\n",
    "    h2 = relu(h2_normed)\n",
    "    \n",
    "    #######################\n",
    "    #  Compute the output layer.\n",
    "    #  We will omit the softmax function here \n",
    "    #  because it will be applied \n",
    "    #  in the softmax_cross_entropy loss\n",
    "    #######################\n",
    "    yhat_linear = nd.dot(h2, W3) + b3\n",
    "    return yhat_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        output = net(data, is_training=False)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Execute the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.131050964008, Train_acc 0.977533, Test_acc 0.9708\n",
      "Epoch 1. Loss: 0.0891214484755, Train_acc 0.9857, Test_acc 0.9769\n",
      "Epoch 2. Loss: 0.0733051626004, Train_acc 0.990283, Test_acc 0.9776\n",
      "Epoch 3. Loss: 0.0481838382682, Train_acc 0.993233, Test_acc 0.9789\n",
      "Epoch 4. Loss: 0.0439620076941, Train_acc 0.994983, Test_acc 0.9808\n",
      "Epoch 5. Loss: 0.035822861736, Train_acc 0.9964, Test_acc 0.9796\n",
      "Epoch 6. Loss: 0.0315828234747, Train_acc 0.997017, Test_acc 0.9824\n",
      "Epoch 7. Loss: 0.0262876220982, Train_acc 0.997367, Test_acc 0.9808\n",
      "Epoch 8. Loss: 0.0210992084311, Train_acc 0.997333, Test_acc 0.9793\n",
      "Epoch 9. Loss: 0.0173964529426, Train_acc 0.99875, Test_acc 0.9831\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "learning_rate = .001\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            # we are in training process,\n",
    "            # so we normalize the data using batch mean and variance\n",
    "            output = net(data, is_training=True)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if i == 0:\n",
    "            moving_loss = nd.mean(loss).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
    "            \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Compared with the pure mlp and dropout results, we achieve over 97% accuracy on this task **even just after the first epoch**, with just two hidden layers containing 256 and 128 hidden nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
